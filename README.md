# End-to-End Data Engineering Project: dbt, Snowflake & Apache Airflow

## ğŸš€ Overview

This project represents a **complete end-to-end data engineering pipeline**, leveraging the power of **dbt (Data Build Tool)** for transformation, **Snowflake** for cloud warehousing, and **Apache Airflow** for orchestration. It covers the full data lifecycle â€” from ingestion and transformation to testing and scheduling â€” following modern data stack best practices.

## ğŸ§° Tech Stack

* **dbt Core** â€“ SQL-based data transformation and modeling
* **Snowflake** â€“ Scalable, cloud-based data warehouse
* **Apache Airflow** â€“ Workflow scheduling and orchestration
* **Python** â€“ Used for scripting and automation (Airflow DAGs)
* **Git** â€“ Version control and collaboration

## ğŸ—‚ï¸ Project Structure

```bash
snowflake_data_project/
â”œâ”€â”€ Data_Files/                # Raw input data in CSV format
â”œâ”€â”€ models/                   # dbt models: staging and marts layers
â”‚   â”œâ”€â”€ staging/              # Cleansed layer
â”‚   â”œâ”€â”€ marts/                # Final business metrics
â”‚   â””â”€â”€ example/              # sources.yml for raw data sources
â”œâ”€â”€ tests/                    # dbt test configs
â”œâ”€â”€ seeds/                    # Seed files (optional static data)
â”œâ”€â”€ snapshots/                # (Future use) Data snapshots for slowly changing dimensions
â”œâ”€â”€ dbt_core_dag.py           # Airflow DAG to run dbt workflow
â”œâ”€â”€ dbt_project.yml           # dbt project configuration file
â”œâ”€â”€ .gitignore                # Git ignored files
â”œâ”€â”€ .gitattributes            # Git attribute settings
â””â”€â”€ README.md                 # Project documentation
```

## âš™ï¸ Setup & Installation

### 1. ğŸ“¥ Clone the Repository

```bash
git clone https://github.com/1AyaNabil1/End-to-End_Data_Engineering_Project.git
cd End-to-End_Data_Engineering_Project
```

### 2. ğŸ Set Up a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate    # Mac/Linux
venv\Scripts\activate       # Windows
```

### 3. ğŸ” Configure dbt Connection to Snowflake

Update the `~/.dbt/profiles.yml` file with your credentials:

```yaml
snowflake_data_project:
  outputs:
    dev:
      account: <your_snowflake_account>
      user: dbt_user
      password: <your_password>
      role: ACCOUNTADMIN
      database: finance_db
      warehouse: finance_wh
      schema: raw
      type: snowflake
  target: dev
```

### 4. â–¶ï¸ Run dbt Models

```bash
dbt run           # Build models

# Optional: run tests

dbt test          # Run data quality tests
```

### 5. ğŸ“… Start Apache Airflow (Local)

```bash
airflow standalone  # Launches webserver and scheduler
```

## âœ… Features Implemented

* Raw CSV ingestion defined in `sources.yml`
* Staging models for: customers, orders, products, and order\_items
* Mart model: `fct_daily_order_revenue`
* Automated data validation via dbt tests (e.g., accepted values, uniqueness)
* Scheduled execution via Airflow DAG (`dbt_core_dag.py`)

## ğŸ“Œ Notes

* Make sure your Airflow environment has access to dbt CLI.
* Replace placeholder credentials before running.
* Extend the DAG with more granular dbt tasks if needed.

---

Made with â¤ï¸ by Aya | [GitHub](https://github.com/1AyaNabil1)
